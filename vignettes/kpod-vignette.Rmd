---
title: "Introduction to kpodcluster"
output: 
  html_notebook:
    code_folding: hide
    toc: true
    toc_float: true
    toc_depth: 4
    number_sections: true
    theme: readable
    highlight: tango
    fig.width: 3
    fig.height: 2
---

***

# Why k-POD?

***

## K-means drawback

***

One of the most common clustering algorithms used by data scientists today is k-means clustering. K-means clustering is an easy-to-follow and fast-to-execute algorithm to fulfill basic clustering needs with a simple function call, `kmeans()`. 

However, k-means suffers one major drawback : it requires a __complete__ data set without missing entries. Realistically, in many data analysis scenarios, data sets tend to have missing entries due to errors in data collection or data cleaning. When attempting to perform k-means clustering on data sets with missing entries, one faces the following options before proceeding to the clustering:

***
1. eliminate the observations(rows) with missing features(columns) from the data set, then perform k-means clustering on partial data;

2. impute the missing values using a certain algorithm, then perform k-means clustering on imputed data;

3. contact the data collecting agency for clarification on the missing values.

***

## K-POD prevails

***

All these options are either computationally expensive, or lacking competence in producing the ideal clustering results. The `kpodcluster` package's `kpod()` offers an simple, reliable and fast alternative to resolve the difficulties of applying k-means clustering on __incomplete__ data. 

In addition to resolving the incomplete data issue, `kpod()` users have the default option to use the __k-means++__ algorithm to initialize centers, in order to address the potentially inconsistent clustering results caused by randomized initial centers in k-means clustering.

***

# Example Data

***

## makeData()

***

`kpodclustr` includes a `makeData()` function that generates data sets with missing entries under user's customization. It produces a list object consisting of 3 objects:

***

- `Orig` : the complete data set.

- `Missing` : the incomplete data set. 

- `truth` : the cluster assignment matrix. 

***

For the remainder of the demonstration of `kpodcluster`, we will use the example data generated from the following code. 

```{r exampleData, class.source = 'fold-show'}
library(kpodclustr)
testData <- kpodclustr::makeData(p = 2, n = 20, k = 5, sigma = 0.25, missing = 0.3, seed = 1991); testData
```

## Parameters example: 

***

- n = 20 (20 observations)

- p = 2 (each observation has 2 features)

- k = 5 (5 clusters, 5 centroids) 

- sigma = 0.25 (scalar of variance for noise around the centroids) 

- missing = 0.3 (missingness, or percentage of missing entries. 20 * 2 * 0.3 = 12 missing entries)

- seed = 1991 (random seed for reproducibility)

***

## Process of `makeData()`:

***

1. Generate centroid matrix, `M`, with `k`*`p` random samples from standard normal distribution.

2. Generate assignment matrix, `assignment`, with `n` ramdom samples from 1 to `k` with replacement.

3. Initialize complete data matrix, `X`, using `M` and `assignment`, so that each observation of `X` has the same features as its assigned cluster centroid.

4. Add noise to each value in the initialized `X` with standard normal distribution multiplying a deviation scalar of `sigma`. 

5. Initialize incomplete data matrix, `X_missing`, as a copy of `X`.

6. Generate indices of missing entries, `missing_ix`, with `n`\*`p`\*`missing` random samples from 1 to `n`\*`p` without replacement.

7. Replace values in `X_missing` at `missing_ix` indices with `NA`.

8. Return `X`, `X_missing`, and `assignment`.

***

# Usage of k-POD 

***

## On example data

***

The __key function__ in `kpodclustr` is `kpod()`. `kpod()` delegates to the rest of the functions in `kpodclustr`. Like `kmeans()`, `kpod()` has a data set parameter, and a number-of-clusters parameter. The difference is that `kpod()` takes in a incomplete data set. The following code and output displays the usage of `kpod()` on the second object in the Example Data list, `X_missing`.

```{r kpodUsage, class.source = 'fold-show'}
kpodclustr::kpod(X = testData[[2]], k = 5)
```

## Returned items explained

***

- `cluster` : The final cluster assignment matrix containing a sequence of cluster labels corresponding to each observation.

- `cluster_list` : A list of cluster assignment matrices throughout all iterations until clustering converges or `kpod()` reaches maximum iterations. If clustering converges, the last two cluster assignment matrices should be the same. `cluster` is the last object in `cluster_list`.

- `obj_vals` : The calculated value of the k-means objective function throughout iterations. The __k-means objective function__ measures the sum of all "intra-cluster variance", or "within-cluster sum of square", which is the sum of squared distance between each observation and its cluster center for all observations. K-means aims to minimize this intra-cluster variance. As shown in the example output, the objective function value decreases as k-POD clustering progresses. 

- `fit` : The final measurement of fit of cluster assignment after clustering commences. The closer the fit is to 1, the better the fit. The fit equation is explained below this section.

- `fit_list` : A list of measurement of fit of cluster assignment throughout all iterations until clustering converges or `kpod()` reaches maximum iterations. `fit` is the last object in `fit_list`. As shown in the example output, the fit increases and approaches 1 as k-POD clustering progresses.

***

## The `fit` equation

***

<center> $fit = 1 - ((\sum withinss) / (totss))$ </center>

***

As mentioned in explanation of `obj_vals`, "withinss" means "within-cluster sum of square", which is the sum of squared distance between each observation and its cluster center for each cluster. Taking the sum of "withinss" yields the sum of squared distance between each observation and its cluster center for all clusters.

There is also "betweenss", meaning "between-cluster sum of square" or "inter-cluster variance", is the sum of squared distance between all cluster centroids. 

"totss", meaning "total sum of square", is the sum of squared distance between each observation and the global center(the center of all observations). 

A related identity: 

***

<center> $totss = \sum withinss + betweenss$ </center>

***

# k-means vs k-POD

> The k-POD method builds upon k-means clustering to provide a simple and quick alternative to clustering missing data that works even when _the missingness mechanism is unknown_, when _external information is unavailable_, and when _there is significant missingness in the data_. --- "k-POD: A Method for k-Means Clustering of Missing Data" by Jocelyn T. Chi, Eric C. Chi & Richard G. Baraniuk

## Performance comparison

***

In this clustering performance comparison between k-means and k-POD, we will focus on 2 metrics: fit and Adjusted Rand Index (ARI). 

The fit metric is explained under "Usage of k-POD". 

The ARI metric is a measure of similarity between 2 data clusterings. The maximum ARI is 1, meaning the 2 clustering results are the same. The lower the ARI, the greater the difference between 2 data clusterings. The "adjusted" aspect takes into account the permutations of cluster labels. For example, clustering result [1,2,3,3,1] is the same as [2,3,1,1,3]. Therefore, the ARI between those two clusterings is 1. 

__Notice__: The scatter plots for k-POD performance will include all observations in the complete data, because the point of k-POD is to perform clustering on all the observations without eliminating the observations with missing features. However, in k-POD, those missing features will be assigned values and updated throughout iterations, which are not likely the same values as the ones in the complete data. 

***

### Data 1

***

Data 1 parameters: p = 2, n = 20, k = 3, sigma = 0.25, seed = 1991

Data 1 is a relatively small data set (20 observations). We will be looking at:
- k-means performance with 0% missing; 
- k-POD performance with 10% missing; 
- k-POD performance with 20% missing; 
- k-POD performance with 30% missing; 
- k-POD performance with 40% missing; 
- and k-POD performance with 50% missing.

They are demonstrated by the following scatter plots, colored by cluster group. Black dots in the k-means plot represents the cluster centers.  


```{r data1means, out.height="50%", out.width="50%"}
library(ggplot2)
data1 <- kpodclustr::makeData(p = 2, n = 20, k = 3, sigma = 0.25, missing = 0.1, seed = 1991)
X1991 <- data1[[1]]
km1991 <- kmeans(X1991,3)
fit_km1991 <- 1-(sum(km1991$withinss)/km1991$totss)

g1991means <- ggplot(data = as.data.frame(X1991), mapping = aes(x=X1991[,1],y=X1991[,2], color=factor(km1991$cluster))) + geom_point() + theme(legend.position = "none") 
g1991means <- g1991means + geom_point(data = as.data.frame(km1991$centers), mapping = aes(x=km1991$centers[,1],y=km1991$centers[,2],size = 3), color = "black")
g1991means <- g1991means + labs(x = "Feature 1", y = "Feature 2", title = "Data 1, k-means with complete data")
g1991means 
```

```{r data1pod0.1}
Xm1991 <- data1[[2]]
kp1991 <- kpodclustr::kpod(Xm1991,3)

fit_kp1991 <- rep(0,5)
fit_kp1991[1] <- kp1991$fit

library(fossil)
ari_kp1991 <- rep(0,5)
ari_kp1991[1] <- adj.rand.index(km1991$cluster, kp1991$cluster)

g1991pod <- ggplot(data = as.data.frame(X1991), mapping = aes(x=X1991[,1],y=X1991[,2], color=factor(kp1991$cluster))) + geom_point() + theme(legend.position = "none")
g1991pod <- g1991pod + labs(x = "Feature 1", y = "Feature 2", title = "Data 1, k-POD with 10% missingness")
g1991pod
```

```{r data1pod0.2}
data1 <- kpodclustr::makeData(p = 2, n = 20, k = 3, sigma = 0.25, missing = 0.2, seed = 1991)

Xm1991 <- data1[[2]]
kp1991 <- kpodclustr::kpod(Xm1991,3)

fit_kp1991[2] <- kp1991$fit
ari_kp1991[2] <- adj.rand.index(km1991$cluster, kp1991$cluster)

g1991pod <- ggplot(data = as.data.frame(X1991), mapping = aes(x=X1991[,1],y=X1991[,2], color=factor(kp1991$cluster))) + geom_point() + theme(legend.position = "none")
g1991pod <- g1991pod + labs(x = "Feature 1", y = "Feature 2", title = "Data 1, k-POD with 20% missingness")
g1991pod

```

```{r data1pod0.3}
data1 <- kpodclustr::makeData(p = 2, n = 20, k = 3, sigma = 0.25, missing = 0.3, seed = 1991)
Xm1991 <- data1[[2]]
kp1991 <- kpodclustr::kpod(Xm1991,3)

fit_kp1991[3] <- kp1991$fit
ari_kp1991[3] <- adj.rand.index(km1991$cluster, kp1991$cluster)

g1991pod <- ggplot(data = as.data.frame(X1991), mapping = aes(x=X1991[,1],y=X1991[,2], color=factor(kp1991$cluster))) + geom_point() + theme(legend.position = "none")
g1991pod <- g1991pod + labs(x = "Feature 1", y = "Feature 2", title = "Data 1, k-POD with 30% missingness")
g1991pod
```

```{r data1pod0.4}
data1 <- kpodclustr::makeData(p = 2, n = 20, k = 3, sigma = 0.25, missing = 0.4, seed = 1991)

Xm1991 <- data1[[2]]
kp1991 <- kpodclustr::kpod(Xm1991,3)

fit_kp1991[4] <- kp1991$fit
ari_kp1991[4] <- adj.rand.index(km1991$cluster, kp1991$cluster)

g1991pod <- ggplot(data = as.data.frame(X1991), mapping = aes(x=X1991[,1],y=X1991[,2], color=factor(kp1991$cluster))) + geom_point() + theme(legend.position = "none")
g1991pod <- g1991pod + labs(x = "Feature 1", y = "Feature 2", title = "Data 1, k-POD with 40% missingness")
g1991pod
```

```{r data1pod0.5}
data1 <- kpodclustr::makeData(p = 2, n = 20, k = 3, sigma = 0.25, missing = 0.5, seed = 1991)

Xm1991 <- data1[[2]]
kp1991 <- kpodclustr::kpod(Xm1991,3)

fit_kp1991[5] <- kp1991$fit
ari_kp1991[5] <- adj.rand.index(km1991$cluster, kp1991$cluster)

g1991pod <- ggplot(data = as.data.frame(X1991), mapping = aes(x=X1991[,1],y=X1991[,2], color=factor(kp1991$cluster))) + geom_point() + theme(legend.position = "none")
g1991pod <- g1991pod + labs(x = "Feature 1", y = "Feature 2", title = "Data 1, k-POD with 50% missingness")
g1991pod
```
***

The first line graph demonstrates the change in fit value as missingness changes. The blue line represents the fit of the k-means clustering. 

The second line graph demonstrates the change in ARI value as missingness changes. 

```{r data1metrics}
missingpcts <- c(0.1,0.2,0.3,0.4,0.5)
metrics_list <- cbind(missingpcts,fit_kp1991,ari_kp1991)

g1991podfit <- ggplot(data = as.data.frame(metrics_list), mapping = aes(x=missingpcts,y=fit_kp1991)) + geom_line() + geom_point() + geom_label(aes(label = round(fit_kp1991,4)), nudge_y = -0.1)
g1991podfit <- g1991podfit + geom_hline(mapping = aes(yintercept = fit_km1991), color = "blue")
g1991podfit <- g1991podfit + labs(x = "Missingness", y = "Fit") + ylim(0,1)
g1991podfit

g1991podari <- ggplot(data = as.data.frame(metrics_list), mapping = aes(x=missingpcts,y=ari_kp1991)) + geom_line() + geom_point() + geom_label(aes(label = round(ari_kp1991,4)), nudge_y = -0.1)
g1991podari <- g1991podari + labs(x = "Missingness", y = "ARI") + ylim(0,1)
g1991podari

```


### Data 2

Data 2: p = 2, n = 1000, k = 3, sigma = 0.25, seed = 1991

Data 2 is a larger data set (1000 observations) than Data 1. We will be looking at:
- k-means performance with 0% missing; 
- k-POD performance with 10% missing; 
- k-POD performance with 20% missing; 
- k-POD performance with 30% missing; 
- k-POD performance with 40% missing; 
- and k-POD performance with 50% missing.

They are demonstrated by the following scatter plots, colored by cluster group. Black dots in the k-means plot represents the cluster centers. 

```{r data2means}
data2 <- kpodclustr::makeData(p = 2, n = 1000, k = 3, sigma = 0.25, missing = 0.1, seed = 1991)
X1991 <- data2[[1]]
km1991 <- kmeans(X1991,3)

fit_km1991 <- 1-(sum(km1991$withinss)/km1991$totss)

g1991means <- ggplot(data = as.data.frame(X1991), mapping = aes(x=X1991[,1],y=X1991[,2], color=factor(km1991$cluster))) + geom_point() + theme(legend.position = "none") 
g1991means <- g1991means + geom_point(data = as.data.frame(km1991$centers), mapping = aes(x=km1991$centers[,1],y=km1991$centers[,2], size = 3), color = "black")
g1991means <- g1991means + labs(x = "Feature 1", y = "Feature 2", title = "Data 2, k-means with complete data")
g1991means 
```
Comment: This example happen to demonstrate the inconsistencies in k-means clustering!

```{r data2pod0.1}
Xm1991 <- data2[[2]]
kp1991 <- kpodclustr::kpod(Xm1991,3)

fit_kp1991 <- rep(0,5)
fit_kp1991[1] <- kp1991$fit

ari_kp1991 <- rep(0,5)
ari_kp1991[1] <- adj.rand.index(km1991$cluster, kp1991$cluster)
g1991pod <- ggplot(data = as.data.frame(X1991), mapping = aes(x=X1991[,1],y=X1991[,2], color=factor(kp1991$cluster))) + geom_point() + theme(legend.position = "none")
g1991pod <- g1991pod + labs(x = "Feature 1", y = "Feature 2", title = "Data 2, k-POD with 10% missingness")
g1991pod
```

```{r data2pod0.2}
data2 <- kpodclustr::makeData(p = 2, n = 1000, k = 3, sigma = 0.25, missing = 0.2, seed = 1991)

Xm1991 <- data2[[2]]
kp1991 <- kpodclustr::kpod(Xm1991,3)

fit_kp1991[2] <- kp1991$fit
ari_kp1991[2] <- adj.rand.index(km1991$cluster, kp1991$cluster)

g1991pod <- ggplot(data = as.data.frame(X1991), mapping = aes(x=X1991[,1],y=X1991[,2], color=factor(kp1991$cluster))) + geom_point() + theme(legend.position = "none")
g1991pod <- g1991pod + labs(x = "Feature 1", y = "Feature 2", title = "Data 2, k-POD with 20% missingness")
g1991pod
```

```{r data2pod0.3}
data2 <- kpodclustr::makeData(p = 2, n = 1000, k = 3, sigma = 0.25, missing = 0.3, seed = 1991)
Xm1991 <- data2[[2]]
kp1991 <- kpodclustr::kpod(Xm1991,3)

fit_kp1991[3] <- kp1991$fit
ari_kp1991[3] <- adj.rand.index(km1991$cluster, kp1991$cluster)

g1991pod <- ggplot(data = as.data.frame(X1991), mapping = aes(x=X1991[,1],y=X1991[,2], color=factor(kp1991$cluster))) + geom_point() + theme(legend.position = "none")
g1991pod <- g1991pod + labs(x = "Feature 1", y = "Feature 2", title = "Data 2, k-POD with 30% missingness")
g1991pod
```

```{r data2pod0.4}
data2 <- kpodclustr::makeData(p = 2, n = 1000, k = 3, sigma = 0.25, missing = 0.4, seed = 1991)

Xm1991 <- data2[[2]]
kp1991 <- kpodclustr::kpod(Xm1991,3)

fit_kp1991[4] <- kp1991$fit
ari_kp1991[4] <- adj.rand.index(km1991$cluster, kp1991$cluster)

g1991pod <- ggplot(data = as.data.frame(X1991), mapping = aes(x=X1991[,1],y=X1991[,2], color=factor(kp1991$cluster))) + geom_point() + theme(legend.position = "none")
g1991pod <- g1991pod + labs(x = "Feature 1", y = "Feature 2", title = "Data 2, k-POD with 40% missingness")
g1991pod
```

```{r data2pod0.5}
data2 <- kpodclustr::makeData(p = 2, n = 1000, k = 3, sigma = 0.25, missing = 0.5, seed = 1991)

Xm1991 <- data2[[2]]
kp1991 <- kpodclustr::kpod(Xm1991,3)

fit_kp1991[5] <- kp1991$fit
ari_kp1991[5] <- adj.rand.index(km1991$cluster, kp1991$cluster)

g1991pod <- ggplot(data = as.data.frame(X1991), mapping = aes(x=X1991[,1],y=X1991[,2], color=factor(kp1991$cluster))) + geom_point() + theme(legend.position = "none")
g1991pod <- g1991pod + labs(x = "Feature 1", y = "Feature 2", title = "Data 2, k-POD with 50% missingness")
g1991pod
```
***

The first line graph demonstrates the change in fit value as missingness changes. The blue line represents the fit of the k-means clustering. 

The second line graph demonstrates the change in ARI value as missingness changes. 

```{r data2metrics}
missingpcts <- c(0.1,0.2,0.3,0.4,0.5)
metrics_list <- cbind(missingpcts,fit_kp1991,ari_kp1991)

g1991podfit <- ggplot(data = as.data.frame(metrics_list), mapping = aes(x=missingpcts,y=fit_kp1991)) + geom_line() + geom_point() + geom_label(aes(label = round(fit_kp1991,4)), nudge_y = -0.1)
g1991podfit <- g1991podfit + geom_hline(mapping = aes(yintercept = fit_km1991), color = "blue")
g1991podfit <- g1991podfit + labs(x = "Missingness", y = "Fit") + ylim(0,1)
g1991podfit

g1991podari <- ggplot(data = as.data.frame(metrics_list), mapping = aes(x=missingpcts,y=ari_kp1991)) + geom_line() + geom_point() + geom_label(aes(label = round(ari_kp1991,4)), nudge_y = -0.1)
g1991podari <- g1991podari + labs(x = "Missingness", y = "ARI") + ylim(0,1)
g1991podari
```

Comment: Because of the inconsistent k-means clustering, we have a low fit score for the k-means clustering (blue line). We also have low ARI scores because of the drastic difference between the k-means clustering and the k-POD clusterings. 

### Data 3

Data 3: p = 2, n = 2000, k = 5, sigma = 0.25, seed = 1992

```{r data3means}
data3 <- kpodclustr::makeData(p = 2, n = 2000, k = 5, sigma = 0.5, missing = 0.1, seed = 1992)
X1992 <- data3[[1]]
km1992 <- kmeans(X1992,5)

fit_km1992 <- 1-(sum(km1992$withinss)/km1992$totss)

g1992means <- ggplot(data = as.data.frame(X1992), mapping = aes(x=X1992[,1],y=X1992[,2], color=factor(km1992$cluster))) + geom_point() + theme(legend.position = "none") 
g1992means <- g1992means + geom_point(data = as.data.frame(km1992$centers), mapping = aes(x=km1992$centers[,1],y=km1992$centers[,2], size = 3), color = "black")
g1992means <- g1992means + labs(x = "Feature 1", y = "Feature 2", title = "Data 3, k-means with complete data")
g1992means 
```

```{r data3pod0.1}
Xm1992 <- data3[[2]]
kp1992 <- kpodclustr::kpod(Xm1992,5)

fit_kp1992 <- rep(0,5)
fit_kp1992[1] <- kp1992$fit

ari_kp1992 <- rep(0,5)
ari_kp1992[1] <- adj.rand.index(km1992$cluster, kp1992$cluster)
g1992pod <- ggplot(data = as.data.frame(X1992), mapping = aes(x=X1992[,1],y=X1992[,2], color=factor(kp1992$cluster))) + geom_point() + theme(legend.position = "none")
g1992pod <- g1992pod + labs(x = "Feature 1", y = "Feature 2", title = "Data 3, k-POD with 10% missingness")
g1992pod
```
### Summary

***

From the resulting line plots measuring the fit and ARI metrics of k-pod clustering compared to k-means clustering, we can conclude:

- 

These performance tests do not demonstrate k-POD's fast execution time. Details about such attribute of k-POD clustering please refer to "k-POD: A Method for k-Means Clustering of Missing Data" by Jocelyn T. Chi, Eric C. Chi & Richard G. Baraniuk. 

***